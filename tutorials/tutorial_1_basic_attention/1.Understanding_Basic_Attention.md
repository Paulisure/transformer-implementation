# Tutorial 1: Understanding Basic Attention

## Introduction
Welcome to the first tutorial in our series on building a transformer from scratch! In this tutorial, we'll explore the fundamental building block of transformer architectures: the attention mechanism. By the end of this tutorial, you'll understand how attention works and be able to implement it yourself.

## Prerequisites
- Basic understanding of PyTorch
- Familiarity with neural networks
- Python 3.10+ installed
- Poetry for dependency management

## Setup
Before we begin, make sure you have the environment set up:
```bash
# Clone the repository if you haven't already
git clone https://github.com/yourusername/transformer-implementation.git
cd transformer-implementation

# Install dependencies
poetry install

# Activate the virtual environment
poetry shell
```

## 1. Understanding Attention Fundamentals

### What is Attention?
Attention is a mechanism that allows a model to focus on relevant parts of the input when producing each part of the output. Think of it like reading a complex sentence - you might refer back to different parts of the sentence as you try to understand each word's role and meaning.

### Key Components
- **Query (Q)**: The current context from which we're attending (what we're looking for)
- **Key (K)**: The elements we're matching against (what we're comparing to)
- **Value (V)**: The information we want to retrieve
- **Attention Weights**: The importance scores for each value

### The Mathematics
The core attention formula is:
```
Attention(Q, K, V) = softmax(QK^T / √d_k)V
```

Let's break this down:
1. `QK^T`: Compute similarity scores between query and keys
2. `/√d_k`: Scale to prevent extreme softmax values
3. `softmax()`: Convert scores to probabilities
4. `×V`: Weight values by their attention scores

## 2. Implementation Walk-Through

### Basic Attention Implementation
Our implementation in `src/transformer/v1_basic_attention/attention.py` contains two main classes:

1. `ScaledDotProductAttention`: Implements the core attention mechanism
2. `MultiHeadAttention`: Wraps the attention mechanism (we'll use this more in Tutorial 2)

Key implementation details:
```python
def forward(self, query, key, value, mask=None):
    d_k = query.size(-1)
    # Compute attention scores
    attention_scores = torch.matmul(query, key.transpose(-2, -1))
    attention_scores = attention_scores / math.sqrt(d_k)
    
    # Apply mask if provided
    if mask is not None:
        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))
    
    # Convert scores to probabilities
    attention_weights = torch.softmax(attention_scores, dim=-1)
    
    # Compute weighted sum of values
    output = torch.matmul(attention_weights, value)
    return output, attention_weights
```

## 3. Visualizing Attention

To understand how attention works in practice, we'll use our visualization tool. Run:
```bash
poetry run python examples/attention_visualization/visualize.py
```

This will generate several visualizations in the `tutorials/tutorial_1_basic_attention/outputs/` directory:
1. Pattern Recognition Attention
2. Sequential Dependency Attention
3. Fibonacci Sequence Attention

These visualizations demonstrate different aspects of attention behavior, which we'll analyze in detail in the next section.

## 4. Hands-on Experiments

Try modifying the visualization code to explore different attention patterns:

1. Change input patterns:
```python
# Create custom attention patterns
query = torch.zeros(batch_size, seq_length, d_model)
query[0, 0, :] = 1.0  # Focus first token
```

2. Modify attention parameters:
```python
# Adjust scaling factor
attention_scores = attention_scores / (math.sqrt(d_k) * scale_factor)
```

## Next Steps
Continue to the next section, "Understanding Attention Mechanisms: Analysis and Findings," where we'll analyze the visualization outputs in detail and understand what they tell us about attention mechanisms.

## Additional Resources
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original transformer paper
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual guide to transformers
- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) - Deep dive into attention mechanisms